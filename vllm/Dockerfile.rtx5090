FROM nvidia/cuda:12.6.3-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# 필수 패키지 설치
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip python3-dev python3-venv \
    git curl ca-certificates build-essential ninja-build cmake jq \
    ccache libopenblas-dev libomp-dev \
 && rm -rf /var/lib/apt/lists/*

RUN python3 -m pip install --upgrade pip

# RTX 5090 (Blackwell sm_120) 지원을 위한 환경 변수 설정
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;12.0+PTX"
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

# PyTorch를 소스에서 빌드 (sm_120 지원 포함)
RUN git clone --recursive https://github.com/pytorch/pytorch /pytorch && \
    cd /pytorch && \
    git checkout v2.5.1 && \
    git submodule sync && \
    git submodule update --init --recursive

WORKDIR /pytorch

# sm_120 지원을 포함하여 PyTorch 빌드
RUN pip install -r requirements.txt && \
    pip install mkl mkl-include && \
    python3 setup.py install

# vLLM 및 관련 패키지 설치
RUN pip install --no-cache-dir \
    vllm \
    transformers \
    accelerate \
    sentencepiece \
    protobuf \
    ray[serve]

WORKDIR /app

ENV VLLM_LOGGING_LEVEL=INFO

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]