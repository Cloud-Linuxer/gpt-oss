# vLLM Configuration
# Copy this file to .env and customize for your setup

# Hugging Face token for private models
HF_TOKEN=

# Model configuration
MODEL_NAME=openai/gpt-oss-20b
# Alternative models:
# MODEL_NAME=facebook/opt-125m  # For testing with small model
# MODEL_NAME=meta-llama/Llama-2-7b-hf
# MODEL_NAME=mistralai/Mistral-7B-v0.1

# Data type (auto, half, float16, bfloat16)
DTYPE=auto

# Maximum model length
MAX_MODEL_LEN=4096

# GPU memory utilization (0-1)
GPU_MEMORY_UTIL=0.9

# Tensor parallel size (number of GPUs to use)
TP_SIZE=1

# Maximum number of batched tokens
MAX_BATCH_TOKENS=32768

# Maximum number of sequences
MAX_NUM_SEQS=256