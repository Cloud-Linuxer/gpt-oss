FROM nvidia/cuda:12.6.3-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip python3-dev python3-venv \
    git curl ca-certificates build-essential ninja-build cmake jq \
 && rm -rf /var/lib/apt/lists/*

RUN python3 -m pip install --upgrade pip

# RTX 5090을 위한 환경 변수 (sm_89로 에뮬레이션)
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0+PTX"
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn
# RTX 5090을 RTX 4090처럼 인식하도록 설정
ENV CUDA_DEVICE_ORDER=PCI_BUS_ID
ENV PYTORCH_NO_CUDA_MEMORY_CACHING=1

# PyTorch 설치 (CUDA 12.4 지원)
RUN pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124

# vLLM을 소스에서 설치 (최신 버전)
RUN pip install packaging wheel setuptools && \
    git clone https://github.com/vllm-project/vllm.git /vllm && \
    cd /vllm && \
    pip install -e . --no-build-isolation

# 추가 패키지 설치
RUN pip install --no-cache-dir \
    transformers \
    accelerate \
    sentencepiece \
    protobuf \
    ray[serve]

WORKDIR /app

# vLLM 설정
ENV VLLM_LOGGING_LEVEL=INFO
ENV VLLM_ATTENTION_BACKEND=FLASH_ATTN
ENV VLLM_USE_MODELSCOPE=False

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# 시작 스크립트
RUN echo '#!/bin/bash\n\
# RTX 5090 우회 설정\n\
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\
export CUDA_MODULE_LOADING=LAZY\n\
python3 -m vllm.entrypoints.openai.api_server "$@"' > /entrypoint.sh && \
chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]