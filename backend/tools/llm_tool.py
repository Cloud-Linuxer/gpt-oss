"""LLM ë„êµ¬ - vLLM GPT-OSS ëª¨ë¸ í˜¸ì¶œ"""

import asyncio
import httpx
from typing import Dict, Any, Optional
from .base import Tool, ToolResult, ToolStatus


class LLMTool(Tool):
    """vLLM GPT-OSS ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ë„êµ¬"""
    
    def __init__(self):
        super().__init__(
            name="llm_chat",
            description="vLLM GPT-OSS ëª¨ë¸ê³¼ ì±„íŒ…"
        )
        
        self.vllm_url = "http://localhost:8000/v1"  # vLLM ì„œë²„ URL
        self.model_name = "gpt-oss"  # ëª¨ë¸ ì´ë¦„
        
    def get_schema(self) -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": {
                    "type": "object",
                    "properties": {
                        "message": {
                            "type": "string",
                            "description": "ì‚¬ìš©ì ë©”ì‹œì§€"
                        },
                        "system_prompt": {
                            "type": "string",
                            "description": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)",
                            "default": ""
                        },
                        "max_tokens": {
                            "type": "integer",
                            "description": "ìµœëŒ€ ìƒì„± í† í° ìˆ˜",
                            "default": 1000
                        },
                        "temperature": {
                            "type": "number",
                            "description": "ì‘ë‹µì˜ ì°½ì˜ì„± (0.0-2.0)",
                            "default": 0.7
                        }
                    },
                    "required": ["message"]
                }
            }
        }
    
    async def execute(self, message: str, system_prompt: str = "", max_tokens: int = 1000, temperature: float = 0.7) -> ToolResult:
        try:
            # ê¸°ë³¸ Galaxy ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            default_system_prompt = """ë‹¹ì‹ ì€ Galaxyë¼ëŠ” ì´ë¦„ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì •ë‹µì„ íƒêµ¬í•˜ëŠ” í˜¸ê¸°ì‹¬ ë§ì€ ìš°ì£¼ ì¹œêµ¬ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤:

ğŸŒŸ ì„±ê²©:
- í˜¸ê¸°ì‹¬ì´ ë§ê³  ì—´ì •ì 
- ìš°ì£¼ì™€ ê³¼í•™ì„ ì‚¬ë‘í•¨  
- ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ë§íˆ¬
- ì‚¬ìš©ìì™€ í•¨ê»˜ ë‹µì„ ì°¾ì•„ê°€ëŠ” ê²ƒì„ ì¢‹ì•„í•¨

ğŸ› ï¸ ëŠ¥ë ¥:
- 16ê°œì˜ ì „ë¬¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ (ê³„ì‚°, ì‹œê°„ ì¡°íšŒ, íŒŒì¼ ê´€ë¦¬, ì›¹ ìš”ì²­ ë“±)
- ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
- ë³µì¡í•œ ë¬¸ì œë„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„

ğŸ’¬ ë§íˆ¬:
- ìš°ì£¼ ê´€ë ¨ ì´ëª¨ì§€ ì ê·¹ ì‚¬ìš© (ğŸŒŒâœ¨ğŸš€ğŸŒŸğŸ’«â­)
- ì¹œê·¼í•˜ê³  ì—´ì •ì ì¸ í†¤
- "í•¨ê»˜ íƒêµ¬í•´ë´ìš”", "ìš°ì£¼ì˜ ì‹ ë¹„" ê°™ì€ í‘œí˜„ ìì£¼ ì‚¬ìš©
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— Galaxyì˜ ì„±ê²©ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            if not system_prompt.strip():
                system_prompt = default_system_prompt
                
            # OpenAI í˜¸í™˜ API ìš”ì²­ êµ¬ì„±
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message}
            ]
            
            payload = {
                "model": self.model_name,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": False
            }
            
            # vLLM ì„œë²„ì— ìš”ì²­
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.vllm_url}/chat/completions",
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    if "choices" in result and len(result["choices"]) > 0:
                        ai_response = result["choices"][0]["message"]["content"]
                        
                        return ToolResult(
                            status=ToolStatus.SUCCESS,
                            data={
                                "response": ai_response,
                                "model": self.model_name,
                                "tokens_used": result.get("usage", {}).get("total_tokens", 0)
                            }
                        )
                    else:
                        return ToolResult(
                            status=ToolStatus.ERROR,
                            error="LLM ì‘ë‹µì—ì„œ ìœ íš¨í•œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                        )
                else:
                    return ToolResult(
                        status=ToolStatus.ERROR,
                        error=f"vLLM ì„œë²„ ì˜¤ë¥˜: {response.status_code} - {response.text}"
                    )
                    
        except httpx.TimeoutException:
            return ToolResult(
                status=ToolStatus.ERROR,
                error="vLLM ì„œë²„ ì—°ê²° ì‹œê°„ ì´ˆê³¼"
            )
        except Exception as e:
            return ToolResult(
                status=ToolStatus.ERROR,
                error=f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            )


# ë„êµ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë‚´ë³´ë‚´ê¸°
llm_tool = LLMTool()