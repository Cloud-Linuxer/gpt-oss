# CLAUDE.md - Project Configuration for Claude

## Project Overview
GPT/LLM deployment project with ArgoCD/GitOps setup for AI model serving.

## System Specifications
- **CPU**: AMD Ryzen 9 9950X3D 16-Core (32 threads, up to 5.75 GHz)
- **RAM**: 60GB
- **GPU**: NVIDIA GeForce RTX 5090 (32GB VRAM, CUDA 12.9)
- **Storage**: 1.9TB (1.5TB available)
- **OS**: Fedora Linux 42 Server Edition
- **Kernel**: 6.15.10

## Project Structure
```
/home/gpt-oss/
â”œâ”€â”€ backend/       # Backend services
â”œâ”€â”€ frontend/      # Frontend application
â”œâ”€â”€ vllm/         # VLLM configuration
â”œâ”€â”€ models/       # Model storage
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ build-vllm-local.sh
```

## Development Commands
```bash
# Build VLLM locally
./build-vllm-local.sh

# Docker compose operations
docker-compose up -d
docker-compose down

# Make commands
make build
make run
make clean
```

## Linting and Type Checking
```bash
# Backend (if Python)
cd backend && ruff check .
cd backend && mypy .

# Frontend (if Node.js)
cd frontend && npm run lint
cd frontend && npm run typecheck
```

## Testing
```bash
# Backend tests
cd backend && pytest

# Frontend tests  
cd frontend && npm test
```

## Git Workflow
- Main branch: `main`
- Commit message format: Descriptive message focusing on "why"
- Always run linting and tests before committing

## Important Notes
- This is an ArgoCD/GitOps managed project
- Models directory contains AI model files
- VLLM is configured for vector LLM operations
- System has high-end GPU suitable for AI/ML workloads

## Docker Build Optimization ì‘ì—… ë‚´ì—­

### ì„±ê³µ ì‚¬ë¡€
1. **ì˜ì¡´ì„± ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±** (download-deps.sh)
   - PyTorch ë° ê´€ë ¨ íŒ¨í‚¤ì§€ wheel íŒŒì¼ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ êµ¬í˜„
   - ìºì‹œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì • (deps/wheels/)

2. **ë¹ ë¥¸ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±** (build-fast.sh) 
   - ìë™ìœ¼ë¡œ ì˜ì¡´ì„± í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
   - Docker BuildKit ìºì‹œ í™œìš© ì„¤ì •

3. **Python ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°**
   - Python 3.13 â†’ 3.11 â†’ 3.10 ìˆœì°¨ì  í•´ê²°
   - Docker ì´ë¯¸ì§€ì˜ Python ë²„ì „ í™•ì¸ í›„ ë§ì¶¤í˜• wheel ë‹¤ìš´ë¡œë“œ

4. **PyTorch ë²„ì „ ì¶©ëŒ í•´ê²°**
   - torch 2.5.1ê³¼ í˜¸í™˜ë˜ëŠ” torchaudio 2.5.1, torchvision 0.20.1 ë²„ì „ ë§¤ì¹­
   - CUDA 12.4 ë²„ì „ìš© wheel íŒŒì¼ ì‚¬ìš©

5. **ì—¬ëŸ¬ Dockerfile ë³€í˜• ìƒì„±**
   - Dockerfile.fast: ì‚¬ì „ ë‹¤ìš´ë¡œë“œ wheel í™œìš©
   - Dockerfile.offline: ê°„ë‹¨í•œ ì ‘ê·¼ ë°©ì‹
   - Dockerfile.simple: ìµœì†Œ Python ê¸°ë°˜ ì´ë¯¸ì§€

### ì‹¤íŒ¨ ì‚¬ë¡€
1. **Python 3.13 wheel í˜¸í™˜ì„± ë¬¸ì œ**
   - ì›ì¸: ì‹œìŠ¤í…œ Python 3.13ê³¼ Docker ì´ë¯¸ì§€ Python ë²„ì „ ë¶ˆì¼ì¹˜
   - í•´ê²°: Docker ì´ë¯¸ì§€ì— ë§ëŠ” Python 3.10 wheel ë‹¤ìš´ë¡œë“œ

2. **PyTorch ì»´í¬ë„ŒíŠ¸ ë²„ì „ ì¶©ëŒ**
   - ì›ì¸: torchaudio 2.6.0ì´ torch 2.6.0 ìš”êµ¬í•˜ë‚˜ 2.5.1 ì„¤ì¹˜ë¨
   - í•´ê²°: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ 2.5.1 ë²„ì „ìœ¼ë¡œ í†µì¼

3. **ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ ì‹œë„**
   - ì›ì¸: ì‚¬ìš©ìê°€ ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ ëŒ€ì‹  ì§ì ‘ wheel ì‚¬ìš© ìš”ì²­
   - í•´ê²°: ë‹¨ìˆœí•œ COPY ë°©ì‹ìœ¼ë¡œ ë³€ê²½

4. **Docker ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì†ë„ ë¬¸ì œ**
   - NVIDIA CUDA ë² ì´ìŠ¤ ì´ë¯¸ì§€ í¬ê¸°: 1.44GB + 2.53GB
   - ì—¬ì „íˆ ì§„í–‰ ì¤‘ì¸ ë¬¸ì œ, ë¡œì»¬ ìºì‹±ìœ¼ë¡œ ë¶€ë¶„ í•´ê²°

### ì¶”ê°€ ë°œê²¬ ì‚¬í•­
5. **vLLM ì˜ì¡´ì„± ì¶©ëŒ**
   - ë¬¸ì œ: vLLMì´ torch 2.7.1ì„ ìš”êµ¬í•˜ì—¬ ê¸°ì¡´ 2.5.1ê³¼ ì¶©ëŒ
   - í•´ê²°: vLLM ì„¤ì¹˜ ì‹œ ìë™ìœ¼ë¡œ torch 2.7.1ë¡œ ì—…ê·¸ë ˆì´ë“œë¨
   - ê²°ê³¼: ë¹Œë“œ ì„±ê³µ

### ìµœì¢… ê²°ê³¼
âœ… **ë¹Œë“œ ì„±ê³µ!**
- ì´ë¯¸ì§€ ì´ë¦„: `vllm-rtx5090:fast`
- ì´ë¯¸ì§€ í¬ê¸°: 18.9GB
- ë¹Œë“œ ì‹œê°„: ì•½ 9ë¶„ (550.4ì´ˆ)
- ì£¼ìš” íŒ¨í‚¤ì§€ ë²„ì „:
  - torch: 2.7.1
  - vllm: 0.10.1.1
  - CUDA: 12.6
  - Python: 3.10

### ë¹Œë“œ ìµœì í™” íš¨ê³¼
- ì‚¬ì „ ë‹¤ìš´ë¡œë“œëœ wheel íŒŒì¼ í™œìš©ìœ¼ë¡œ ì´ˆê¸° PyTorch ì„¤ì¹˜ ì‹œê°„ ë‹¨ì¶•
- Docker ë ˆì´ì–´ ìºì‹±ìœ¼ë¡œ ì¬ë¹Œë“œ ì‹œ ì†ë„ í–¥ìƒ
- ìµœì¢… ì´ë¯¸ì§€ í¬ê¸°: 18.9GB (ê¸°ì¡´ 34.4GB ëŒ€ë¹„ ì•½ 45% ê°ì†Œ)

### RTX 5090 í˜¸í™˜ì„± ë¬¸ì œ
âš ï¸ **ì¤‘ìš”í•œ ì œí•œì‚¬í•­**
- ë¬¸ì œ: RTX 5090 (Blackwell, sm_120)ê³¼ PyTorch 2.7.1 ë¹„í˜¸í™˜
- ì˜¤ë¥˜ ë©”ì‹œì§€: "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible"
- ì›ì¸: PyTorch 2.7.1ì€ í˜„ì¬ sm_50~sm_90ê¹Œì§€ë§Œ ì§€ì›

### RTX 5090 í•´ê²° ì‹œë„ í˜„í™©

#### 1. í™˜ê²½ ë³€ìˆ˜ ìš°íšŒ ì‹œë„ (Dockerfile.rtx5090-fix) ğŸ”„
- RTX 5090ì„ RTX 4090 (sm_89)ë¡œ ì—ë®¬ë ˆì´ì…˜
- í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:
  - `TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0+PTX"`
  - `PYTORCH_NO_CUDA_MEMORY_CACHING=1`
  - `VLLM_WORKER_MULTIPROC_METHOD=spawn`
- vLLM ì†ŒìŠ¤ ë¹Œë“œë¡œ ìµœì‹  ì§€ì› í™•ë³´
- ìƒíƒœ: ë¹Œë“œ ì§„í–‰ ì¤‘

#### 2. PyTorch Nightly ë¹Œë“œ (Dockerfile.nightly) âŒ
- ìµœì‹  ê°œë°œ ë²„ì „ ì‚¬ìš© ì‹œë„
- ê²°ê³¼: ì—¬ì „íˆ sm_120 ë¯¸ì§€ì›ìœ¼ë¡œ ì‹¤íŒ¨

#### 3. PyTorch ì†ŒìŠ¤ ë¹Œë“œ (Dockerfile.source-build) ğŸ”„
- PyTorchë¥¼ ì†ŒìŠ¤ì—ì„œ ì§ì ‘ ì»´íŒŒì¼
- `TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;10.0;12.0+PTX"` ì„¤ì •
- ê°€ì¥ í¬ê´„ì ì¸ í•´ê²°ì±…ì´ë‚˜ ë¹Œë“œ ì‹œê°„ ì¥ì‹œê°„ ì†Œìš”
- ìƒíƒœ: ë¹Œë“œ ì§„í–‰ ì¤‘

#### 4. CPU ëª¨ë“œ ì‚¬ìš© âŒ
- ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ê±°ë¶€ ("cpu ëª¨ë“œëŠ” í•„ìš”ì—†ì–´")

### ë‹¤ìŒ ë‹¨ê³„
- ì§„í–‰ ì¤‘ì¸ ë¹Œë“œ ì™„ë£Œ ëŒ€ê¸° ë° í…ŒìŠ¤íŠ¸
- PyTorch ê³µì‹ Blackwell ì§€ì› ì—…ë°ì´íŠ¸ ëª¨ë‹ˆí„°ë§
- NVIDIA ë° PyTorch ë ˆí¬ì§€í† ë¦¬ ì´ìŠˆ íŠ¸ë˜í‚¹