# RTX 5090 + Qwen3-Next-80B ë°°í¬ ì‹œë„ ë³´ê³ ì„œ
## 1ì°¨ ì‹œë„ - 2025ë…„ 9ì›” 15ì¼

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ëª©í‘œ
RTX 5090 (Blackwell sm_120) GPUì—ì„œ Qwen3-Next-80B-A3B-Instruct ëª¨ë¸ì„ vLLMìœ¼ë¡œ ë°°í¬

---

## ğŸ”§ ì‹œìŠ¤í…œ ì‚¬ì–‘
### í•˜ë“œì›¨ì–´
- **GPU**: NVIDIA RTX 5090 (32GB VRAM, sm_120 Blackwell ì•„í‚¤í…ì²˜)
- **CPU**: 32 cores
- **RAM**: 60GB
- **Storage**: 1.9TB (1.3TB available)
- **OS**: Linux 6.15.10-200.fc42.x86_64

### ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½
- **Docker**: Latest
- **CUDA**: 12.8.0
- **Python**: 3.11

---

## ğŸš€ êµ¬í˜„ ë‚´ìš©

### 1. RTX 5090 ì§€ì› Docker ì´ë¯¸ì§€ ë¹Œë“œ (âœ… ì„±ê³µ)

#### Dockerfile.rtx5090
```dockerfile
FROM nvidia/cuda:12.8.0-devel-ubuntu22.04

# PyTorch nightly + CUDA 12.8 for RTX 5090 (sm_120) support
RUN pip install --pre torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/nightly/cu128

# Environment variables for RTX 5090
ENV TORCH_CUDA_ARCH_LIST="7.0;7.5;8.0;8.6;8.9;9.0;12.0+PTX"
ENV CUDA_VISIBLE_DEVICES=0
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Build vLLM from source with sm_120 support
RUN git clone https://github.com/vllm-project/vllm.git /vllm && \
    cd /vllm && \
    pip install ninja packaging && \
    TORCH_CUDA_ARCH_LIST="7.0;7.5;8.0;8.6;8.9;9.0;12.0+PTX" \
    MAX_JOBS=4 \
    pip install -v .
```

**ë¹Œë“œ ê²°ê³¼**:
- ì´ë¯¸ì§€ í¬ê¸°: 51.6GB
- ë¹Œë“œ ì‹œê°„: ì•½ 3ì‹œê°„
- vLLM ì»´íŒŒì¼: 501ê°œ CUDA ì»¤ë„ ì„±ê³µì ìœ¼ë¡œ ë¹Œë“œ
- FlashAttention 3 í¬í•¨

### 2. Docker Compose ì„¤ì •

#### docker-compose.qwen.yml (ì£¼ìš” ë¶€ë¶„)
```yaml
services:
  qwen3-next:
    image: qwen3-rtx5090:latest
    container_name: qwen3-next-vllm
    runtime: nvidia
    environment:
      - TORCH_CUDA_ARCH_LIST=7.0;7.5;8.0;8.6;8.9;9.0;12.0+PTX
      - VLLM_USE_TRITON_FLASH_ATTN=1
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - CUDA_GRAPH_DISABLE=1
      - VLLM_DISABLE_CUSTOM_ALL_REDUCE=1
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model Qwen/Qwen3-Next-80B-A3B-Instruct
      --dtype auto
      --gpu-memory-utilization 0.90
      --cpu-offload-gb 80
      --max-model-len 8192
      --tensor-parallel-size 1
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --enforce-eager
      --disable-custom-all-reduce
```

### 3. ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

#### deploy-rtx5090.sh
```bash
#!/bin/bash
# RTX 5090 optimized deployment for Qwen3-Next-80B

echo "ğŸ” Checking RTX 5090..."
nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv,noheader

echo "ğŸ”¨ Building RTX 5090 optimized Docker image..."
docker build -f Dockerfile.rtx5090 -t qwen3-rtx5090:latest .

echo "ğŸš€ Starting services..."
docker compose -f docker-compose.qwen.yml up -d
```

---

## âŒ ë¬¸ì œì  ë° ì‹¤íŒ¨ ì›ì¸

### 1. ë©”ëª¨ë¦¬ ë¶€ì¡±
**ëª¨ë¸ ìš”êµ¬ì‚¬í•­ vs ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤**:
```
ëª¨ë¸ ìš”êµ¬ì‚¬í•­:
- ëª¨ë¸ íŒŒë¼ë¯¸í„°: 160GB (80B Ã— 2 bytes BF16)
- KV ìºì‹œ: 10-20GB
- í™œì„±í™” ë©”ëª¨ë¦¬: 5-10GB
- ì´ í•„ìš”: 175-190GB

ì‹œìŠ¤í…œ ê°€ìš©:
- GPU VRAM: 32GB
- System RAM: 60GB
- ì´ ê°€ìš©: 92GB

ë¶€ì¡±ë¶„: 83-98GB
```

### 2. ëª¨ë¸ ì•„í‚¤í…ì²˜ ë³µì¡ì„±
- **MoE êµ¬ì¡°**: 512ê°œ ì „ë¬¸ê°€ ëª¨ë¸ (10ê°œë§Œ í™œì„±í™”ë˜ì§€ë§Œ ëª¨ë‘ ë©”ëª¨ë¦¬ì— ë¡œë“œ)
- **Hybrid ì•„í‚¤í…ì²˜**: DeltaNet + Attention ë™ì‹œ ì‚¬ìš©
- **ì´ˆì¥ë¬¸ ì»¨í…ìŠ¤íŠ¸**: ê¸°ë³¸ 262K í† í° ì§€ì› (ì¼ë°˜ ëª¨ë¸ì˜ 64ë°°)

### 3. ì‹œë„ëœ ìµœì í™” (íš¨ê³¼ ì—†ìŒ)
- CPU ì˜¤í”„ë¡œë”© 80GB ì„¤ì •
- CUDA Graph ë¹„í™œì„±í™”
- Eager mode ì‹¤í–‰
- ë©”ëª¨ë¦¬ í™œìš©ë¥  90% ì„¤ì •

---

## ğŸ’¡ í•´ê²° ë°©ì•ˆ

### 1. Quantization ì ìš©
- **INT8**: ë©”ëª¨ë¦¬ 50% ì ˆê° (80GB í•„ìš”)
- **INT4/GPTQ**: ë©”ëª¨ë¦¬ 75% ì ˆê° (40GB í•„ìš”) âœ… ê¶Œì¥

### 2. ì‘ì€ ëª¨ë¸ ì‚¬ìš©
- **Qwen2.5-14B-Instruct**: ~28GB âœ… í˜„ì¬ ì‹œìŠ¤í…œì— ì í•©
- **Qwen2.5-7B-Instruct**: ~14GB âœ… í˜„ì¬ ì‹œìŠ¤í…œì— ì í•©
- **Qwen2.5-32B-Instruct**: ~64GB (CPU ì˜¤í”„ë¡œë”© í•„ìš”)

### 3. í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ
- RAM 128GB ì´ìƒìœ¼ë¡œ ì¦ì„¤
- ì¶”ê°€ GPU ì„¤ì¹˜ (tensor parallel)

---

## ğŸ“Š ì„±ê³¼ ë° êµí›ˆ

### ì„±ê³¼
1. âœ… RTX 5090 (sm_120) ì§€ì› vLLM ì„±ê³µì ìœ¼ë¡œ ë¹Œë“œ
2. âœ… PyTorch nightly + CUDA 12.8 í™˜ê²½ êµ¬ì„± ì™„ë£Œ
3. âœ… FlashAttention 3 í†µí•© ì„±ê³µ
4. âœ… Docker ì´ë¯¸ì§€ ìµœì í™” ì™„ë£Œ (51.6GB)

### êµí›ˆ
1. MoE ëª¨ë¸ì€ í™œì„± íŒŒë¼ë¯¸í„°ë³´ë‹¤ ì „ì²´ íŒŒë¼ë¯¸í„° ê¸°ì¤€ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê³„ì‚° í•„ìš”
2. RTX 5090ì€ ìµœì‹  ì•„í‚¤í…ì²˜ì§€ë§Œ VRAM 32GBëŠ” ëŒ€í˜• ëª¨ë¸ì— ë¶€ì¡±
3. CPU ì˜¤í”„ë¡œë”©ë„ ì‹œìŠ¤í…œ RAMì´ ì¶©ë¶„í•´ì•¼ íš¨ê³¼ì 

---

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„

1. **ì¦‰ì‹œ ê°€ëŠ¥**: Qwen2.5-14B ë˜ëŠ” 7B ëª¨ë¸ë¡œ ì „í™˜
2. **ì¤‘ê¸° ê³„íš**: Qwen3-Next-80Bì˜ INT4 quantized ë²„ì „ ì‹œë„
3. **ì¥ê¸° ê³„íš**: RAM ì—…ê·¸ë ˆì´ë“œ í›„ ì¬ì‹œë„

---

## ğŸ“ ê´€ë ¨ íŒŒì¼
- `/home/gpt-oss/Dockerfile.rtx5090`
- `/home/gpt-oss/docker-compose.qwen.yml`
- `/home/gpt-oss/deploy-rtx5090.sh`
- `/home/gpt-oss/vllm_qwen3_startup.py`
- `/home/gpt-oss/.env` (HuggingFace í† í° í¬í•¨)

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ
- **í”„ë ˆì„ì›Œí¬**: vLLM 0.10.2rc3
- **ì»´íŒŒì¼ëŸ¬**: CUDA 12.8 + PyTorch nightly
- **ìµœì í™”**: FlashAttention 3, Triton backend
- **ì»¨í…Œì´ë„ˆ**: Docker + Docker Compose
- **GPU ì§€ì›**: sm_120 (RTX 5090 Blackwell)

---

*ì‘ì„±ì¼: 2025ë…„ 9ì›” 15ì¼*
*ì‘ì„±ì: Claude Code Assistant*
*í”„ë¡œì íŠ¸: RTX 5090 LLM Deployment*