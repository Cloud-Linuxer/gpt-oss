version: '3.8'

services:
  # vLLM Model Server
  vllm:
    image: vllm/vllm-openai:v0.10.1.1
    container_name: gpt-oss-vllm
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_LOGGING_LEVEL=INFO
    command: >
      python3 -m vllm.entrypoints.openai.api_server
      --model openai/gpt-oss-20b
      --dtype auto
      --gpu-memory-utilization 0.8
      --host 0.0.0.0
      --port 8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Backend API with Tools
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: gpt-oss-backend:latest
    container_name: gpt-oss-backend
    ports:
      - "8080:8080"
      - "8001:8001"  # Tool proxy port
    environment:
      - ENV=production
      - HOST=0.0.0.0
      - PORT=8080
      - LOG_LEVEL=INFO
      - VLLM_BASE_URL=http://vllm:8000
      - VLLM_MODEL=openai/gpt-oss-20b
      - VLLM_MAX_TOKENS=1000
      - VLLM_TEMPERATURE=0.7
      - VLLM_TIMEOUT=60
      - PYTHONUNBUFFERED=1
    volumes:
      - ./backend:/app
      - ./data:/data  # For file tools access
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    command: >
      sh -c "
      python proxy_with_tools.py &
      uvicorn app:app --host 0.0.0.0 --port 8080 --reload
      "

  # Streamlit Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: gpt-oss-frontend:latest
    container_name: gpt-oss-frontend
    ports:
      - "8501:8501"
    environment:
      - BACKEND_URL=http://backend:8080
      - TOOL_PROXY_URL=http://backend:8001
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_SERVER_FILE_WATCHER_TYPE=none
      - STREAMLIT_THEME_PRIMARY_COLOR=#FF4B4B
      - STREAMLIT_THEME_BACKGROUND_COLOR=#0E1117
      - STREAMLIT_THEME_SECONDARY_BACKGROUND_COLOR=#262730
      - STREAMLIT_THEME_TEXT_COLOR=#FAFAFA
    volumes:
      - ./frontend:/app
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    command: streamlit run frontend_integrated.py

  # Optional: Redis for caching (performance improvement)
  redis:
    image: redis:7-alpine
    container_name: gpt-oss-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Optional: Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    container_name: gpt-oss-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    restart: unless-stopped

volumes:
  redis_data:
    driver: local

networks:
  default:
    name: gpt-oss-network
    driver: bridge