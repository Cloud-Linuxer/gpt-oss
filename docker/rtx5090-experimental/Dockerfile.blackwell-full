# RTX 5090 (sm_120) 완전 지원 Dockerfile
FROM nvidia/cuda:12.8.0-devel-ubuntu22.04

# 기본 패키지
RUN apt-get update && apt-get install -y \
    python3.11 python3.11-dev python3-pip \
    git curl wget build-essential ninja-build \
    && rm -rf /var/lib/apt/lists/*

RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1
RUN python -m pip install --upgrade pip setuptools wheel

# PyTorch with CUDA 12.8
RUN pip install torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu128

# sm_120 환경 변수
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;12.0"
ENV MAX_JOBS=8
ENV CUDA_HOME=/usr/local/cuda

# FlashAttention 소스 빌드 (sm_120 포함)
RUN pip install packaging ninja wheel
RUN git clone https://github.com/Dao-AILab/flash-attention.git /flash-attention && \
    cd /flash-attention && \
    pip install -v . --no-build-isolation

# vLLM 소스 빌드 (sm_120 포함)
RUN git clone https://github.com/vllm-project/vllm.git /vllm && \
    cd /vllm && \
    git checkout v0.11.0 && \
    TORCH_CUDA_ARCH_LIST="12.0" pip install -v . --no-build-isolation

# 환경 설정
ENV HF_HOME=/models
ENV TRANSFORMERS_CACHE=/models
RUN mkdir -p /models /app
WORKDIR /app

EXPOSE 8000
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "openai/gpt-oss-20b", \
     "--dtype", "auto", \
     "--gpu-memory-utilization", "0.90", \
     "--max-model-len", "8192", \
     "--host", "0.0.0.0", \
     "--port", "8000", \
     "--trust-remote-code"]
