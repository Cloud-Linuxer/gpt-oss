version: '3.8'

services:
  # vLLM Model Server
  vllm:
    image: vllm/vllm-openai:v0.10.1.1
    container_name: gpt-oss-vllm
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "0.0.0.0:8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_LOGGING_LEVEL=INFO
    command: >
      --model openai/gpt-oss-20b
      --dtype auto
      --gpu-memory-utilization 0.8
      --host 0.0.0.0
      --port 8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Backend API with Tools
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: gpt-oss-backend:latest
    container_name: gpt-oss-backend
    ports:
      - "0.0.0.0:8080:8080"
      - "0.0.0.0:8001:8001"
    environment:
      - ENV=production
      - HOST=0.0.0.0
      - PORT=8080
      - LOG_LEVEL=INFO
      - VLLM_BASE_URL=http://vllm:8000
      - VLLM_MODEL=openai/gpt-oss-20b
      - VLLM_MAX_TOKENS=1000
      - VLLM_TEMPERATURE=0.7
      - VLLM_TIMEOUT=60
      - PYTHONUNBUFFERED=1
    volumes:
      - ./backend:/app
      - ./data:/data
    depends_on:
      - vllm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Streamlit Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: gpt-oss-frontend:latest
    container_name: gpt-oss-frontend
    ports:
      - "0.0.0.0:8501:8501"
    environment:
      - BACKEND_URL=http://backend:8080
      - TOOL_PROXY_URL=http://backend:8001
      - PROXY_URL=http://backend:8001/v1/chat/completions
      - PROXY_BASE=http://backend:8001
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_SERVER_FILE_WATCHER_TYPE=none
    volumes:
      - ./frontend:/app
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    command: streamlit run frontend_integrated.py

networks:
  default:
    name: gpt-oss-network
    driver: bridge